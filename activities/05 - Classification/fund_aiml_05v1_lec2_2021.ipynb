{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fund_aiml_05v1_lec2_2021.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNl91y5YdhkSMbHBgPNoGLe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dyjdlopez/fund-of-aiml/blob/main/activities/05%20-%20Classification/fund_aiml_05v1_lec2_2021.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDPzQ-B7jlCr"
      },
      "source": [
        "# Topic 05.2: Non-Linear Classification\n",
        "$_{\\text{©D.J. Lopez | 2021 | Fundamentals of Machine Learning}}$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0keqpG0ojgp0"
      },
      "source": [
        "### Part 1: Perceptron Algorithm\n",
        "The Perceptron was first conceptualized by Frank Rosenblatt in his paper [The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain](https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.335.3398&rep=rep1&type=pdf) in 1958. The perceptron is one of the earliest mathematical conceptualization of a brain neuron. In simplest terms, a perceptron does a weighted sum of all inputs and then performs an activation. In the early implementations of the perceptron the activation used was a step function described as:\n",
        "$$step(z) = \\left\\{\n",
        "  \\begin{array}\\\\\n",
        "    1 \\text{ if } \\ b+ \\sum w_iX_n\\geq 0 \\\\\n",
        "    0 \\text{ otherwise}\n",
        "    \\end{array}\n",
        "\\right.\n",
        "$$\n",
        "![image](https://jontysinai.github.io/assets/article_images/2017-11-11-the-perceptron/bio-vs-MCP.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qf8u6x3nkUj6"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rho0Jc7jjHiB"
      },
      "source": [
        "\n",
        "N = 100\n",
        "cov = 0\n",
        "center1 = np.array([0, 1.7]).T\n",
        "cov_mat1 = np.array([\n",
        "                    [1,cov],\n",
        "                    [cov,1]\n",
        "])\n",
        "\n",
        "center2 = np.array([0, -1.7]).T\n",
        "cov_mat2 = np.array([\n",
        "                    [1,cov],\n",
        "                    [cov,1]\n",
        "])\n",
        "\n",
        "np.random.seed(0)\n",
        "X1 = np.array(np.random.multivariate_normal(center1,cov_mat1,int(N/2)))\n",
        "X2 = np.array(np.random.multivariate_normal(center2,cov_mat2,int(N/2)))\n",
        "## We'll declare another gaussian distribution with a different mean\n",
        "\n",
        "plt.figure(figsize=(8,8))\n",
        "plt.scatter(X1[:,0],X1[:,1], c='r', label='1')\n",
        "plt.scatter(X2[:,0],X2[:,1], c='b', label='0')\n",
        "plt.ylabel(\"Some Target\")\n",
        "plt.xlabel(\"Some Feature\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6XxnCkyej3WT"
      },
      "source": [
        "def step_activation(z):\n",
        "  \"\"\"\n",
        "  Compute the step activation of z\n",
        "\n",
        "  Arguments:\n",
        "  z -- A scalar or numpy array of any size.\n",
        "\n",
        "  Return:\n",
        "  filtered step activations step(z)\n",
        "  \"\"\"\n",
        "  return np.where(z>=0, 1,0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOIqNkFkj67a"
      },
      "source": [
        "def init_weights(dim):\n",
        "  \"\"\"\n",
        "  Does a zero-initialization of the weights and bias\n",
        "\n",
        "  Arguments:\n",
        "  dim -- Desired dimension for the weights.\n",
        "\n",
        "  Return:\n",
        "  w -- initialized weights\n",
        "  b -- initilaized bias\n",
        "  \"\"\"\n",
        "  w = np.zeros(shape=(dim,1))\n",
        "  b = 0\n",
        "  return w, b"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fElfMzdZj8pV"
      },
      "source": [
        "def sum_err(preds,y):\n",
        "  \"\"\"\n",
        "  Computes the Sum of Squared Errors for a set of predictions\n",
        "  and truth values\n",
        "\n",
        "  Arguments:\n",
        "  preds -- Set of predictions.\n",
        "  y -- Set of truth values\n",
        "\n",
        "  Return:\n",
        "  sse -- Sum of the squared errors\n",
        "  \"\"\"\n",
        "  sse = np.sum(np.square(y-preds))\n",
        "  return sse"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_0BcXuqj_GS"
      },
      "source": [
        "def accuracy(preds, Y):\n",
        "  \"\"\"\n",
        "  Computes the accuracy for a set of predictions\n",
        "  and truth values\n",
        "\n",
        "  Arguments:\n",
        "  preds -- Set of predictions.\n",
        "  y -- Set of truth values\n",
        "\n",
        "  Return:\n",
        "  accuracy -- Computed accuracy\n",
        "  \"\"\"\n",
        "  accuracy = 1-np.mean(np.abs(preds-Y))\n",
        "  return accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVXVGAl6kDgD"
      },
      "source": [
        "def propagate(X,y,w,b):\n",
        "\n",
        "  # Compute for the transformed vector of the \n",
        "  # dataset w.r.t the weights and biases\n",
        "  z = (X@w) + b\n",
        "\n",
        "  # Compute for the step activation\n",
        "  A = step_activation(z)\n",
        "\n",
        "  # Compute for the prediction error\n",
        "  error = A-y\n",
        "  acc = accuracy(y,A)\n",
        "\n",
        "  # Update the weights and biases\n",
        "  # Learning/Update routine\n",
        "  w = np.dot(X.T,error)\n",
        "  b = np.sum(error)\n",
        "\n",
        "  # Compute the cost\n",
        "  cost = sum_err(A,y)\n",
        "\n",
        "  # Store the parameters in a dictionary for tracking\n",
        "  grads = {\"dw\": w,\n",
        "           \"db\": b}\n",
        "  \n",
        "  return grads, cost, acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPGwNd4AkFVz"
      },
      "source": [
        "w,b = init_weights(X_train.shape[1])\n",
        "propagate(X_train,y_train,w,b)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAy-JGQnkIA3"
      },
      "source": [
        "def train(w, b, X, y, lr, epochs, early_stopping=True, stop_thresh=0.9):\n",
        "  costs = []\n",
        "  accuracies = []\n",
        "\n",
        "  for i in range(epochs):\n",
        "    # Do a forward propagation to obtain the gradients\n",
        "    grads, cost, accuracy = propagate(X,y,w,b)\n",
        "\n",
        "    # Locally store the gradients    \n",
        "    dw=grads['dw']\n",
        "    db=grads['db']\n",
        "\n",
        "    # Update routine per epoch\n",
        "    w = w - lr*dw\n",
        "    b = b - lr*db\n",
        "\n",
        "    # Store the costs per epoch for logs      \n",
        "    \n",
        "    if i % 20 == 0:\n",
        "      print (f\"Epoch {i}: Loss: {cost} Accuracy: {accuracy}\")\n",
        "      costs.append(cost)\n",
        "      accuracies.append(accuracy)\n",
        "\n",
        "    # Store the learned parameters for logs\n",
        "    params = {\"w\": w,\n",
        "              \"b\": b}\n",
        "    \n",
        "    grads = {\"dw\": dw,\n",
        "             \"db\": db}\n",
        "    if early_stopping and accuracy >= stop_thresh:\n",
        "      print(f\"Target metric met, stopping the training at {i} epoch(s).\\n\")\n",
        "      break\n",
        "\n",
        "  return params, grads, costs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVspdyK0kK87"
      },
      "source": [
        "w,b = init_weights(X_train.shape[1])\n",
        "learning_rate = 1\n",
        "epochs = 100\n",
        "\n",
        "params, grads, costs = train(w, b, X_train, y_train, \n",
        "                             lr=learning_rate, epochs=epochs, \n",
        "                             early_stopping=True, stop_thresh=1.0)\n",
        "\n",
        "print (\"w = \" + str(params[\"w\"]))\n",
        "print (\"b = \" + str(params[\"b\"]))\n",
        "print (\"dw = \" + str(grads[\"dw\"]))\n",
        "print (\"db = \" + str(grads[\"db\"]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFrVtpYRkNOh"
      },
      "source": [
        "def predict(X, weights, bias):\n",
        "  z = (X@weights)+bias\n",
        "  return np.where(z>=0, 1,0)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72ni2U5NkOsb"
      },
      "source": [
        "weights = params[\"w\"]\n",
        "bias = params[\"b\"]\n",
        "preds = predict(X_test,weights,bias)\n",
        "accuracy(y_test, preds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GxtYuhOGkRcM"
      },
      "source": [
        "\n",
        "\n",
        "c_matrix = confusion_matrix(y_test, preds)\n",
        "sns.heatmap(c_matrix, annot=True)\n",
        "plt.xlabel(\"Ground Truths\")\n",
        "plt.ylabel(\"Predicted\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7EWdii5GlDa7"
      },
      "source": [
        "## Part 2: Back-propagation\n",
        "Although the Perceptron with the step activation produces good results producing a linear classifier, it lacks another fundamental technique for being a robust neural network model—Backpropagation. Backpropagation is a short form for \"backward propagation of errors.\" It is a method of training artificial neural networks. This method helps to calculate the gradient of a loss function for all the weights in the network. <br>\n",
        "In this section, we will use a sigmoid function as an activation function instead of a step activation. Since backpropagation will not be effective with the step function its gradient (derivative) is zero, and that will not be useful for computing the loss function.<br>\n",
        "<b>Loss Function</b><br>\n",
        "A loss function is the function we want to minimize or maximize is called the objective function or criterion. When we are minimizing it, we may also call it the cost function, loss function, or error function [[1]](https://www.deeplearningbook.org/contents/numerical.html). <br>\n",
        "To save you the time and brainpower, for our example our loss function is:\n",
        "$$J(\\theta)=\\frac{1}{m} \\sum^m_{i}cost(h_{\\theta}(x^{(i)}, y^{(i)}) \\\\\n",
        "\\text{if y = 1} : -\\log{(h_\\theta(x))}\\\\\n",
        "\\text{if y = 0} : -\\log{(1-h_\\theta(x))}\\\\\n",
        "J(\\theta)=-\\frac{1}{m} \\sum^m_{i}{y^{(i)}\\log{(h_\\theta(x))}+(1-y^{(i)})(\\log{(1-h_\\theta(x))}} \\\\\n",
        "J(\\theta)=-\\frac{1}{m} \\sum^m_{i}{Y^T\\log(h)+(1-Y)^T\\log(1-h)}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mlh2YqwFlNoi"
      },
      "source": [
        "def sigmoid(z):\n",
        "    \"\"\"\n",
        "    Compute the sigmoid of z\n",
        "\n",
        "    Arguments:\n",
        "    z -- A scalar or numpy array of any size.\n",
        "\n",
        "    Return:\n",
        "    s -- sigmoid(z)\n",
        "    \"\"\"\n",
        "    s = 1 / (1 + np.exp(-z))    \n",
        "    return s"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "454Z9kellOtC"
      },
      "source": [
        "def transfer_derivative(d):\n",
        "  return d*(1.0-d)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPQvdRm5lRtz"
      },
      "source": [
        "def propagate(w, b, X, Y):\n",
        "    \"\"\"\n",
        "    Implement the cost function and its gradient for the propagation explained above\n",
        "\n",
        "    Arguments:\n",
        "    w -- weights, a numpy array \n",
        "    b -- bias, a scalar\n",
        "    X -- data of size\n",
        "    Y -- true \"label\" vector \n",
        "\n",
        "    Return:\n",
        "    cost -- negative log-likelihood cost for logistic regression\n",
        "    dw -- gradient of the loss with respect to w, thus same shape as w\n",
        "    db -- gradient of the loss with respect to b, thus same shape as b\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    m = X.shape[0]\n",
        "    alpha = 10**-8\n",
        "    \n",
        "    # FORWARD PROPAGATION (FROM X TO COST)\n",
        "    h = sigmoid((X@w)+b)                                   # compute activation\n",
        "    J = -1 / m * np.sum(Y * np.log(h+alpha) + (1-Y) * np.log((1-h)+alpha))  # compute cost\n",
        "    error = (h-Y)*transfer_derivative(h)\n",
        "    # BACKWARD PROPAGATION (TO FIND GRAD)\n",
        "    dw = 1/m * X.T @ error\n",
        "    db = 1/m * np.sum(error)\n",
        "\n",
        "    cost = np.squeeze(J)\n",
        "    \n",
        "    grads = {\"dw\": dw,\n",
        "             \"db\": db}\n",
        "    \n",
        "    return grads, cost"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZDPeyjrlTfK"
      },
      "source": [
        "def optimize(w, b, X, Y, epochs, lr, print_cost = True):\n",
        "    \"\"\"\n",
        "    This function optimizes w and b by running a gradient descent algorithm\n",
        "    \n",
        "    Arguments:\n",
        "    w -- weights, a numpy array of size\n",
        "    b -- bias, a scalar\n",
        "    X -- data of shape \n",
        "    Y -- true \"label\" vector\n",
        "    epochs -- number of iterations of the optimization loop\n",
        "    lr -- learning rate of the gradient descent update rule\n",
        "    print_cost -- True to print the loss every 100 steps\n",
        "    \n",
        "    Returns:\n",
        "    params -- dictionary containing the weights w and bias b\n",
        "    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n",
        "    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n",
        "    \n",
        "    Tips:\n",
        "    You basically need to write down two steps and iterate through them:\n",
        "        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n",
        "        2) Update the parameters using gradient descent rule for w and b.\n",
        "    \"\"\"\n",
        "    \n",
        "    costs = []\n",
        "    \n",
        "    for i in range(epochs):\n",
        "        \n",
        "        \n",
        "        # Cost and gradient calculation \n",
        "        grads, cost = propagate(w, b, X, Y)\n",
        "        \n",
        "        # Retrieve derivatives from grads\n",
        "        dw = grads[\"dw\"]\n",
        "        db = grads[\"db\"]\n",
        "        \n",
        "        w = w - lr * dw\n",
        "        b = b - lr * db\n",
        "        \n",
        "        # Record the costs\n",
        "        if i % 50 == 0:\n",
        "            costs.append(cost)\n",
        "        \n",
        "        # Print the cost every 10 training iterations\n",
        "        if print_cost and i % 50 == 0:\n",
        "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
        "    \n",
        "    params = {\"w\": w,\n",
        "              \"b\": b}\n",
        "    \n",
        "    grads = {\"dw\": dw,\n",
        "             \"db\": db}\n",
        "    \n",
        "    return params, grads, costs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4o862wW0lV7C"
      },
      "source": [
        "w,b = init_weights(X_train.shape[1])\n",
        "learning_rate = 0.1\n",
        "epochs = 100\n",
        "params, grads, costs = optimize(w, b, X_train, y_train, \n",
        "                             lr=learning_rate, epochs=epochs)\n",
        "\n",
        "print (\"w = \" + str(params[\"w\"]))\n",
        "print (\"b = \" + str(params[\"b\"]))\n",
        "print (\"dw = \" + str(grads[\"dw\"]))\n",
        "print (\"db = \" + str(grads[\"db\"]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0e_Xoi36lYVk"
      },
      "source": [
        "def predict(w, b, X):\n",
        "    '''\n",
        "    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)\n",
        "    \n",
        "    Arguments:\n",
        "    w -- weights, a numpy array of size\n",
        "    b -- bias, a scalar\n",
        "    X -- data of size \n",
        "    \n",
        "    Returns:\n",
        "    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X\n",
        "    '''\n",
        "    \n",
        "    A = sigmoid((X@w)+b) \n",
        "    Y_prediction = np.where(A>=0.5,1,0)\n",
        "    \n",
        "    return Y_prediction"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iK3g355TlZvT"
      },
      "source": [
        "def model(X_train, Y_train, X_test, Y_test, num_iterations = 10, learning_rate = 0.5, print_cost = True):\n",
        "    \"\"\"\n",
        "    Builds the logistic regression model by calling the function you've implemented previously\n",
        "    \n",
        "    Arguments:\n",
        "    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n",
        "    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n",
        "    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n",
        "    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n",
        "    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n",
        "    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n",
        "    print_cost -- Set to true to print the cost every 100 iterations\n",
        "    \n",
        "    Returns:\n",
        "    d -- dictionary containing information about the model.\n",
        "    \"\"\"\n",
        "    \n",
        "    # initialize parameters with zeros\n",
        "    w, b = init_weights(X_train.shape[1])\n",
        "\n",
        "    # Gradient descent \n",
        "    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)\n",
        "    \n",
        "    # Retrieve parameters w and b from dictionary \"parameters\"\n",
        "    w = parameters[\"w\"]\n",
        "    b = parameters[\"b\"]\n",
        "    \n",
        "    Y_prediction_test = predict(w, b, X_test)\n",
        "    Y_prediction_train = predict(w, b, X_train)\n",
        "\n",
        "    # Print train/test Errors\n",
        "    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n",
        "    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n",
        "\n",
        "    \n",
        "    d = {\"costs\": costs,\n",
        "         \"Y_prediction_test\": Y_prediction_test, \n",
        "         \"Y_prediction_train\" : Y_prediction_train, \n",
        "         \"w\" : w, \n",
        "         \"b\" : b,\n",
        "         \"learning_rate\" : learning_rate,\n",
        "         \"num_iterations\": num_iterations}\n",
        "    \n",
        "    return d"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjEruD3-lbeC"
      },
      "source": [
        "neuron_model = model(X_train, y_train, X_test, y_test, num_iterations=100, learning_rate=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gTlO9VYGldak"
      },
      "source": [
        "c_matrix = confusion_matrix(y_test, neuron_model['Y_prediction_test'])\n",
        "sns.heatmap(c_matrix, annot=True)\n",
        "plt.xlabel(\"Ground Truths\")\n",
        "plt.ylabel(\"Predicted\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "619VV-BMlgRq"
      },
      "source": [
        "print(f\"F1 Score: \\t{f1_score(y_test, neuron_model['Y_prediction_test'])}\")\n",
        "print(f\"Recall: \\t{recall_score(y_test, neuron_model['Y_prediction_test'])}\")\n",
        "print(f\"Precision: \\t{precision_score(y_test, neuron_model['Y_prediction_test'])}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VmXm7uY2liQ6"
      },
      "source": [
        "plot_weights(X_train,neuron_model['w'],neuron_model['b'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6PM0SLUkk4q"
      },
      "source": [
        "## Laboratory Activity\n",
        "1. For the laboratory activity, obtain a dataset of your liking from a data source. Explain the purpose of the dataset and mention any publication if it is obtained from the source. Provide a needs statement and significance for the dataset.\n",
        "\n",
        "2. Identify an algorithm or method in performing a single or multiple variable classification using the Perceptron alogrithm. \n",
        "\n",
        "3. You must re-create your Perceptron algorithm with Gradient Descent and Backpropagation using your own code in a separate Google Colab. However, you are required to observe the following:\n",
        "\n",
        ">* Enforce object-oriented programming by implementing at least two of the pillars of OOP in the entirety of the solution.\n",
        "* Dedicated functions for training, predicting, and evaluating the solution.\n",
        "* A DataFrame of the metrics of the solution\n",
        "* A visualization of the solution’s results.\n"
      ]
    }
  ]
}